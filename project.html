<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ICCN Lab - Research</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="icon" href="favicon.png" type="image/png">
</head>
<body>
    <header>
        <nav>
            <div class="container">
                <ul>
                    <li><a href="index.html#home"><em>Home</em></a></li>
                    <li><a href="index.html#about"><em>About</em></a></li>
                    <li><a href="index.html#news"><em>News</em></a></li>
                    <li class="active"><a href="team.html"><em>Team</em></a></li> <!-- Active Page -->
                    <li><a href="research.html"><em>Research</em></a></li>
                    <li><a href="projects.html"><em>Projects</em></a></li>
                    <li><a href="publications.html"><em>Publications</em></a></li>
                    <li><a href="index.html#opportunities"><em>Opportunities</em></a></li>
                    <li><a href="index.html#contact"><em>Contact</em></a></li>
                </ul>
            </div>
        </nav> 

        <div class="container">
            <h1><strong>AI-Native Connectivity, Optimization and Networking Lab</strong></h1>
        </div>
    </header>

    <section id="Project">
        <div class="container">
            <h2>Current Project Topics</h2>
            <hr />
            
            <!-- Project 1: Smart Home Personal Assistant – Emotion-Aware and Contextual IoT Control -->
            <div class="research-area">
                <img src="images/smart_home.png" alt="Smart Home – Emotion-Aware and Contextual Personal Assistant" class="research-image">
                <div class="research-content">
                    <h3>Smart Home – Emotion-Aware and Contextual Personal Assistant</h3>
                    <p>
                        Traditional smart home systems often rely on predefined rules or basic machine learning models, 
                        which struggle to adapt to natural human communication and emotional nuances. To overcome this, 
                        we developed <strong>EchoMate</strong>, a smart home assistant powered by advanced large language models (LLMs) and integrated with 
                        HomeAssistant. Unlike traditional ML systems that require structured inputs and rigid training, LLMs
                        understand flexible, human-like language and complex context—enabling natural, intuitive conversations with users.
                    </p>
                    <p>
                        EcohMate can interpret not just what users say, but how they say it. 
                        It detects emotional cues from voice and adjusts home devices—like lighting, 
                        temperature, or music—accordingly. For example, if someone sounds stressed, EchoMate 
                        may suggest a break and create a soothing environment. Its ability to learn continuously 
                        from user interactions makes it far more adaptable and user-friendly than fixed-rule or task-specific ML systems. 
                        Pilot users report high satisfaction with EcoMate’s personalized responses and seamless interaction, pointing to a more intelligent and emotionally aware future for smart homes.
                    </p>
                </div>
            </div>
            <hr />
            <hr />
            
            <!-- Project 2: Wildfire Early Detection – UAV-Satellite Multimodal Forecasting System -->
            <div class="research-area">
                <img src="images/wildfire.png" alt="Wildfire Early Detection – UAV-Satellite Forecasting System" class="research-image">
                <div class="research-content">
                    <h3>Wildfire Early Detection – UAV-Satellite Forecasting System</h3>
                    <p>
                        Wildfires now threaten over 400 million hectares globally each year, causing immense environmental and economic damage.
                         Many fire-prone areas—especially in remote boreal zones—lack adequate ground-based sensing or rapid-response systems. 
                         To address this market gap, our project introduces an integrated surveillance solution combining unmanned aerial vehicles (UAVs) 
                         with satellite and environmental data. UAVs act as mobile surveillance units, capable of accessing regions where human 
                         deployment is impractical, and autonomously collecting localized multimodal data to complement satellite observations.
                    </p>
                    <p>                         
                         To accommodate the UAVs’ limited onboard resources, we implement a collaborative AI deployment strategy: 
                         lightweight model components operate on the UAVs, while more intensive processing is handled by the control center. 
                         This setup ensures energy-efficient operation while enabling timely forecasts. As the UAVs continuously gather data, 
                         a dynamic database is maintained at the control center to support continuous model updating and adaptation to evolving 
                         wildfire patterns. The system provides valuable support for early intervention and strategic planning, aiming to reduce 
                         fire response times and limit large-scale fire spread.
                    </p>
                
                </div>
            </div>
            <hr />
            <hr />
            
            
            <!-- Project 3: Smart Agriculture – UAV-Based Pest & Disease Detection -->
            <div class="research-area">
                <img src="images/smart_algo.png" alt="Smart Agriculture – UAV-Based Pest & Disease Detection" class="research-image">
                <div class="research-content">
                    <h3>Smart Agriculture – UAV-Based Pest & Disease Detection</h3>
                    <p>
                       As global food demand rises and labor costs increase, 
                       conventional pest scouting covers only ~5–10% of fields—leaving vast blind spots that drive crop losses and pesticide overuse.
                       Despite this urgency, industry lacks scalable, autonomous detection systems that can pinpoint early infestations across large acreages.
                       To address this critical challenge, we developed an autonomous UAV-based system that surveys fields at scale and detects early signs of crop threats using onboard vision and intelligence.

                       By combining high-resolution aerial imagery with robust AI models, our solution enables fast, 
                       accurate identification of pests, diseases, and weeds—down to the plant level. In field pilots, 
                       this system reduced inspection time per acre from hours to minutes, while improving detection precision.
                        As a result, farmers achieved up to 30% reduction in pesticide usage, 20% yield improvement, 
                        and significantly lower operating costs. This approach offers a scalable path toward more sustainable, 
                        efficient, and resilient agriculture.
                    </p>
                </div>
            </div>
            <hr />
            <hr />

            <!-- Project 4: Smart City – Intelligent Traffic Management System -->
            <div class="research-area">
                <img src="images/smart_trans.png" alt="Smart Transportation – UAV-Assisted Intersection Safety System" class="research-image">
                <div class="research-content">
                    <h3>Smart Transportation – UAV-Assisted Intersection Safety System</h3>
                    <p> Urban intersections are high-risk zones for traffic accidents, particularly in environments with mixed mobility—bicycles, 
                        scooters, and lightweight vehicles. Traditional systems often lack real-time awareness and adaptive coordination, 
                        resulting in frequent near-misses. In partnership with <strong>Beware Company</strong>, we are advancing toward a new paradigm: 
                        a 3D smart transportation system powered by UAV-assisted surveillance. 
                    </p> 

                    <p> In this system, UAVs operate as aerial surveillance nodes, creating a real-time view of intersection activity. 
                        These UAVs collaborate with roadside traffic units that act as edge servers, enabling low-latency processing and 
                        localized decision-making. To support efficient communication between UAVs and infrastructure, 
                        we deploy a semantic transmission method—sharing only task-critical information instead of raw streams, 
                        thus ensuring high-throughput, low-bandwidth performance. When potential collision risks are detected, 
                        alerts are sent instantly to users' smartphones. Seamlessly integrating with existing infrastructure, 
                        the system delivers a scalable and intelligent solution for safer, connected mobility
                    </p>

                </div>
            </div>

        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; Intelligent Communication, Computing and Networking Lab. All Rights Reserved.</p>
            <div class="social">
                <a href="#"><i class="fab fa-linkedin"></i></a>
                <a href="#"><i class="fab fa-twitter"></i></a>
                <a href="#"><i class="fab fa-github"></i></a>
            </div>
        </div>
    </footer>
    <script src="script.js"></script>
</body>
</html>
